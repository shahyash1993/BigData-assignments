{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.SparkContext@7136e0e0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@5757f5a4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "/*1. Write a function that will put N doubles into a file. The doubles need to be normally distributed\n",
    "with mean 0 and standard deviation 1. The function should have two arguments: N\n",
    "and the full name of the file (ie includes path to file location).\n",
    "import java.io._*/\n",
    "\n",
    "\n",
    "def doublesInFile(n:Int, filePath:String) : Array[Double] = {\n",
    "    val r = scala.util.Random\n",
    "\n",
    "    var doublesArr:Array[Double] = new Array[Double](n)    //instantiates an array of size n\n",
    "    //print(\"Size: \"+doublesArr.size)\n",
    "\n",
    "    for(i<-0 until n ) {\n",
    "      doublesArr(i) = r.nextGaussian()\n",
    "    }\n",
    "\n",
    "    //writing to the filePath provided\n",
    "    val pw = new PrintWriter(filePath)\n",
    "    val newLine = sys.props(\"line.separator\")              //get the '\\n' for scala\n",
    "\n",
    "    for(i<-doublesArr.indices ){\n",
    "      pw.write(doublesArr(i).toString+newLine)\n",
    "    }\n",
    "    pw.close()\n",
    "    println(\"Successful!\")\n",
    "\n",
    "    doublesArr\n",
    "  }//ebd of def\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array(1.2430155244701695, -1.3183358442110307, 0.47143738676044433, -0.9815809241457173, -1.667834454737898, 0.44355552651587077, -0.49388166209720363, -1.2106055737633865, 1.107426809790712, 0.5985527826794214, 0.3625095620722051, 0.004776243716141819, -0.3842201375394312, 0.7313831451629214, -0.40982748007376607, 1.1157199716652988, 1.3492327583712227, 0.9462376274671586, -0.4439225291400797, 0.1843440247568084, 1.4972259017099598, -0.5007040375924772, -1.102244169309396, -0.257893069548835, -0.6425800010028886, 0.05817705958574153, 0.4218052277262012, -1.290404510837337, -0.2954986126142325, -0.4433706321646232, 0.6658550236796856, 1.2296409030157955, -1.2540185915203448, -0.3563730829533177, 0.480657164972608, -0.06307687175968754, -0.057141479776724936, -0.01..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doublesInFile(50000,\"/home/shahyash1993/bigData/assigment3/doublesFile.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    " /*2. Read the file created in #2 into an RDD and compute the mean and standard deviation of\n",
    "the doubles in the file. Work on the RDD, that is do not convert the RDD to a DataFrame or\n",
    "Dataset.. You are to use Spark code to compute the values as we want this to run on a\n",
    "cluster using multiple machines. So the pure Scala code you used in assignment will not\n",
    "work.*/\n",
    " \n",
    " \n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    " \n",
    "def readRDDMeanSD(): Tuple2[Double,Double]  = {\n",
    "      val filePath = \"/home/shahyash1993/bigData/assigment3/doublesFile.txt\"\n",
    "      val conf = new SparkConf().setAppName(\"readRDDMeanSD\").setMaster(\"local[*]\")\n",
    "\n",
    "      val minPartitions = 2\n",
    "      val doublesData:RDD[Double] = sc.textFile(filePath, minPartitions).map(_.toDouble)   \n",
    "\n",
    "      val mean:Double = doublesData.stats().mean      //using stat to find mean and the standard deviation\n",
    "      val SD:Double = doublesData.stats().stdev\n",
    "      val popSD:Double = doublesData.stats().popStdev\n",
    "\n",
    "      println(\"Mean is: \"+mean + \" & SD is: \"+SD+ \" & popSD is: \"+popSD)\n",
    "      \n",
    "      Tuple2(mean,SD)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean is: -0.0029387081644154257 & SD is: 1.004153011757705 & popSD is: 1.004153011757705\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-0.0029387081644154257,1.004153011757705)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readRDDMeanSD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "/*3. Repeat #3 but using a DataFrame instead of RDD. Here work on the DataFrame not an RDD*/\n",
    "\n",
    "\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.{DataFrame, SQLContext, SparkSession}\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "def usingDF():Unit = {\n",
    "    val filePath = \"/home/shahyash1993/bigData/assigment3/doublesFile.txt\"\n",
    "    val conf = new SparkConf().setAppName(\"usingDF\").setMaster(\"local[*]\")\n",
    "    \n",
    "    val sqlContext: SQLContext = spark.sqlContext\n",
    "\n",
    "    val minPartitions = 2\n",
    "    val doublesRDD:RDD[Double] = sc.textFile(filePath, minPartitions).map(_.toDouble)   //gets the whole RDD elements in Double\n",
    "\n",
    "    import sqlContext.implicits._\n",
    "    val doublesDF = doublesRDD.toDF(\"Numbers\")\n",
    "\n",
    "    println(\"Mean,SD,popSD of the newly generated DataFrame is: \")\n",
    "    \n",
    "    doublesDF.select(avg($\"Numbers\")).show()          \n",
    "    doublesDF.select(stddev($\"Numbers\")).show()\n",
    "    doublesDF.select(stddev_pop($\"Numbers\")).show()\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean,SD,popSD of the newly generated DataFrame is: \n",
      "+--------------------+\n",
      "|        avg(Numbers)|\n",
      "+--------------------+\n",
      "|-0.00293870816441...|\n",
      "+--------------------+\n",
      "\n",
      "+--------------------+\n",
      "|stddev_samp(Numbers)|\n",
      "+--------------------+\n",
      "|   1.004163053438448|\n",
      "+--------------------+\n",
      "\n",
      "+-------------------+\n",
      "|stddev_pop(Numbers)|\n",
      "+-------------------+\n",
      "| 1.0041530117577049|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "usingDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "/*4. Using a DataFrame create a random sample of about 100 elements of the file created in #2\n",
    "and compute the mean of the sample.*/\n",
    "\n",
    "\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.{SQLContext, SparkSession}\n",
    "import org.apache.spark.sql.functions.{avg, stddev, stddev_pop}\n",
    "\n",
    "  def randomSampleMean(): Unit = {\n",
    "\n",
    "    val filePath = \"/home/shahyash1993/bigData/assigment3/doublesFile.txt\"\n",
    "    val conf = new SparkConf().setAppName(\"randomSampleMean\").setMaster(\"local[*]\")\n",
    "    val sqlContext: SQLContext = spark.sqlContext\n",
    "\n",
    "    val doublesDF = spark.read.textFile(filePath)\n",
    "    //val fraction:Float = 100/50000\n",
    "    val sampleFromDF= doublesDF.sample(true, 0.002) //0.002 is taken as it is 100/50000\n",
    "    sampleFromDF.select(mean(sampleFromDF(\"value\"))).show()    \n",
    "  }//end of def\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|         avg(value)|\n",
      "+-------------------+\n",
      "|0.13197156383042807|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "randomSampleMean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "/*5. Create a file of 100 normally distributed doubles. Read the doubles from the file into an\n",
    "RDD. Using the RDD create a sliding window of size 20 and compute the mean of each\n",
    "window.*/\n",
    "\n",
    "\n",
    "import java.io.PrintWriter\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.{SQLContext, SparkSession}\n",
    "\n",
    "def slidingMean(): Unit = {\n",
    "    val n:Int = 100\n",
    "    val slidingSize:Int = 20\n",
    "    val slidingStep:Int = 1\n",
    "\n",
    "    val path:String = \"/home/shahyash1993/bigData/assigment3/doublesFile.txt\"\n",
    "    val r = scala.util.Random\n",
    "    val conf = new SparkConf().setAppName(\"Simple Application\").setMaster(\"local[*]\")\n",
    "    val sqlContext: SQLContext = spark.sqlContext\n",
    "\n",
    "    var doublesArr:Array[Double] = new Array[Double](n)\n",
    "    //print(\"Size: \"+doublesArr.size)\n",
    "\n",
    "    for(i<-0 until n ) {\n",
    "      doublesArr(i) = r.nextGaussian();\n",
    "    }\n",
    "\n",
    "    //write to the file\n",
    "    val printWriter = new PrintWriter(path)\n",
    "    val newLine = sys.props(\"line.separator\")\n",
    "\n",
    "    for(i<-0 until doublesArr.length ){\n",
    "      printWriter.write(doublesArr(i).toString+newLine)\n",
    "    }\n",
    "    printWriter.close()\n",
    "\n",
    "    val minPartitions = 2\n",
    "    val doublesData:RDD[Double] = sc.textFile(path, minPartitions).map(_.toDouble)\n",
    "\n",
    "    //find the mean using sliding and map function\n",
    "    var slidingIterator:Iterator[Double] =  doublesData.collect().sliding(slidingSize,slidingStep).map(x=>x.sum/x.length)\n",
    "\n",
    "    slidingIterator.foreach((d: Double) => println(d))\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10436077218370517\n",
      "0.18394443594753082\n",
      "0.16473827177191153\n",
      "0.20710484721936456\n",
      "0.24147915346032747\n",
      "0.3998787845278735\n",
      "0.36788679242480304\n",
      "0.38869945484381424\n",
      "0.34519815327614173\n",
      "0.41895631426490276\n",
      "0.2800410744192085\n",
      "0.14831839120460172\n",
      "0.08666837877933874\n",
      "-0.048129454341718024\n",
      "-0.012734167506833993\n",
      "-0.007644974040424128\n",
      "0.03589642562373713\n",
      "-0.03058590176773219\n",
      "-0.09045392157258517\n",
      "0.05064692950409153\n",
      "0.02987036522942607\n",
      "-0.08109986262683\n",
      "-0.01566101707374563\n",
      "-0.029023508376274442\n",
      "0.03590138971687899\n",
      "-0.06928944010805489\n",
      "-0.048179870903485134\n",
      "-0.09085911434757891\n",
      "-0.09542432775330627\n",
      "-0.25253484311374563\n",
      "-0.1496502871854543\n",
      "-0.05127417986501544\n",
      "0.004532394633119179\n",
      "0.04114923485571435\n",
      "-0.03342798757690138\n",
      "-0.05293028301332885\n",
      "0.023436073591015905\n",
      "0.010593935581629077\n",
      "0.05966109739567634\n",
      "-0.01914384168750461\n",
      "0.011995042955986945\n",
      "0.0450098059087092\n",
      "-0.01745343338353842\n",
      "0.04374367456759183\n",
      "-0.035355246720924875\n",
      "-0.014197875934041499\n",
      "-0.027116127677916768\n",
      "-0.06703348973072934\n",
      "-0.18386224872316997\n",
      "-0.12700168440319493\n",
      "-0.2051578621084212\n",
      "-0.24089567002420917\n",
      "-0.1684272798474093\n",
      "-0.1248984751822277\n",
      "-0.005692301757231922\n",
      "0.07878607807528241\n",
      "-0.10809428437651776\n",
      "-0.06622439249922855\n",
      "-0.022290551245037517\n",
      "-0.06142742988131189\n",
      "-0.15997402871685057\n",
      "-0.1610483044026199\n",
      "-0.15074619282214644\n",
      "-0.20106078490534793\n",
      "-0.18950411151119928\n",
      "-0.19501292476350027\n",
      "-0.2345842696924066\n",
      "-0.11489968706312359\n",
      "0.0517024411089468\n",
      "0.03238081811488769\n",
      "0.10904478793670094\n",
      "0.1654608409441835\n",
      "0.1178135441497044\n",
      "0.14664088778335369\n",
      "0.050946196221715825\n",
      "0.017426833442555453\n",
      "0.17435285189367553\n",
      "0.1566816369941101\n",
      "0.09722841877898263\n",
      "0.1688917421537126\n",
      "0.2973742922924866\n"
     ]
    }
   ],
   "source": [
    "slidingMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "/*6. The file “multiple-sites.tsv” contains two columns: site and dwell-time. Using Spark compute\n",
    "the average dwell time for each site.*/\n",
    "\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.{SQLContext, SparkSession}\n",
    "\n",
    "def averageDwellTime(): Unit = {\n",
    "    val conf = new SparkConf()\n",
    "    conf.setAppName(\"Word Count\")\n",
    "    conf.setMaster(\"local[2]\")\n",
    "\n",
    "    val sqlContext: SQLContext = spark.sqlContext\n",
    "    import sqlContext.implicits._\n",
    "    val multipleSitesDF = spark.read.format(\"csv\")\n",
    "        .option(\"delimiter\", \"\\t\")\n",
    "        .option(\"header\",true)\n",
    "        .load(\"multiple-sites.tsv\")\n",
    "\n",
    "    val exprs = multipleSitesDF.columns.map(_ -> \"mean\").toMap\n",
    "\n",
    "    println(\"average Dwell-time is: \")\n",
    "    var avg = multipleSitesDF.groupBy($\"site\")\n",
    "                .agg(exprs)\n",
    "                .select(\"site\",\"avg(dwell-time)\")\n",
    "                .show()\n",
    "\n",
    "    //println(\"Schema: \"+multipleSitesDF.printSchema())\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average Dwell-time is: \n",
      "+----+------------------+\n",
      "|site|   avg(dwell-time)|\n",
      "+----+------------------+\n",
      "|   7|123.36734693877551|\n",
      "|  15|119.34782608695652|\n",
      "|  11| 96.98214285714286|\n",
      "|   3| 97.47916666666667|\n",
      "|   8| 94.34693877551021|\n",
      "|  16| 86.74418604651163|\n",
      "|   0| 79.85106382978724|\n",
      "|   5|102.33333333333333|\n",
      "|  18| 94.81481481481481|\n",
      "|  17|  77.8913043478261|\n",
      "|   6|144.19298245614036|\n",
      "|  19| 89.28070175438596|\n",
      "|   9|             89.82|\n",
      "|   1|             106.0|\n",
      "|  10|129.95238095238096|\n",
      "|   4| 94.33333333333333|\n",
      "|  12| 80.95081967213115|\n",
      "|  13| 90.73770491803279|\n",
      "|  14| 74.76470588235294|\n",
      "|   2| 88.22916666666667|\n",
      "+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "averageDwellTime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "/*7. The file “multiple-sites.tsv” contains two columns: date and dwell-time. Using Spark compute\n",
    "the following:\n",
    "1. The average dwell time each hour\n",
    "2. The average dwell time per day of week\n",
    "3. The average dwell time on week-days (Monday - Friday)\n",
    "4. Average dwell time on the weekend.*/\n",
    "\n",
    "\n",
    "def averageDwellTimePro(): Unit = {\n",
    "\n",
    "      val conf = new SparkConf()\n",
    "      conf.setAppName(\"Word Count\")\n",
    "      conf.setMaster(\"local[2]\")\n",
    "\n",
    "      val sqlContext: SQLContext = spark.sqlContext\n",
    "\n",
    "      val dwellTimesDF = sqlContext.read.format(\"csv\")\n",
    "        .option(\"delimiter\", \"\\t\")\n",
    "        .option(\"header\",\"true\")\n",
    "        .load(\"dwell-times.tsv\")\n",
    "        .toDF(\"date\",\"dwell-time\")\n",
    "\n",
    "\n",
    "      import org.apache.spark.sql.functions.{expr, col, column}\n",
    "      import org.apache.spark.sql.functions._\n",
    "\n",
    "//1. The average dwell time each hour\n",
    "      println(\"1. The average dwell time each hour: \")\n",
    "      \n",
    "      val withHour=dwellTimesDF.withColumn(\"hour\", hour(col(\"date\")))\n",
    "        .groupBy(\"hour\")\n",
    "        .agg(mean(\"dwell-time\"))\n",
    "        .sort(col(\"hour\"))\n",
    "        .show()\n",
    "\n",
    "//2. The average dwell time per day of week\n",
    "      println(\"2. The average dwell time per day of week: \")\n",
    "      \n",
    "      val withPerDayOfWeek=dwellTimesDF.withColumn(\"dayOfWeek\", date_format(col(\"date\"),\"EEEE\"))\n",
    "        .groupBy(\"dayOfWeek\")\n",
    "        .agg(mean(\"dwell-time\"))\n",
    "        .show()\n",
    "\n",
    "//3. The average dwell time on week-days (Monday - Friday)\n",
    "      println(\"3. The average dwell time on week-days (Monday - Friday): \")\n",
    "      \n",
    "      val withWeekDays=dwellTimesDF.withColumn(\"weekDays\", date_format(col(\"date\"),\"EEEE\"))\n",
    "        .groupBy(\"weekDays\")\n",
    "        .agg(mean(\"dwell-time\"))\n",
    "        .where(col(\"weekDays\")!==\"Saturday\")\n",
    "        .where(col(\"weekDays\")!==\"Sunday\")\n",
    "        .show()\n",
    "\n",
    "//4. Average dwell time on the weekend.*/\n",
    "      println(\"4. Average dwell time on the weekend: \")\n",
    "      \n",
    "      val withWeekEnd=dwellTimesDF.withColumn(\"weekEnd\", date_format(col(\"date\"),\"EEEE\"))\n",
    "        .groupBy(\"weekEnd\")\n",
    "        .agg(mean(\"dwell-time\"))\n",
    "        .filter(col(\"weekEnd\")===\"Saturday\" || col(\"weekEnd\")===\"Sunday\" )\n",
    "        .show()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average dwell time each hour: \n",
      "+----+-----------------+                                                        \n",
      "|hour|  avg(dwell-time)|\n",
      "+----+-----------------+\n",
      "|   0|  94.708670095518|\n",
      "|   1|92.20954287620954|\n",
      "|   2|96.85937970490816|\n",
      "|   3|92.57110924839341|\n",
      "|   4|91.33086825527253|\n",
      "|   5|96.14621040723982|\n",
      "|   6|92.17173727608757|\n",
      "|   7|94.68557487038731|\n",
      "|   8|92.35551839464883|\n",
      "|   9|90.15113156885309|\n",
      "|  10|92.52882433045846|\n",
      "|  11|92.37820848611838|\n",
      "|  12|91.21507890122736|\n",
      "|  13|92.82834185536888|\n",
      "|  14|95.47408666100254|\n",
      "|  15|90.56982193064667|\n",
      "|  16| 93.1575817641229|\n",
      "|  17|91.66231155778894|\n",
      "|  18|93.13833634719711|\n",
      "|  19| 88.4258114374034|\n",
      "+----+-----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "The average dwell time per day of week: \n",
      "+---------+------------------+\n",
      "|dayOfWeek|   avg(dwell-time)|\n",
      "+---------+------------------+\n",
      "|Wednesday| 91.57283771155643|\n",
      "|  Tuesday| 89.73922367574522|\n",
      "|   Friday| 90.44266729389628|\n",
      "| Thursday|  91.4947995556902|\n",
      "| Saturday|116.88253012048193|\n",
      "|   Monday| 89.11023872679046|\n",
      "|   Sunday|106.49005681818181|\n",
      "+---------+------------------+\n",
      "\n",
      "The average dwell time on week-days (Monday - Friday): \n",
      "+---------+-----------------+\n",
      "| weekDays|  avg(dwell-time)|\n",
      "+---------+-----------------+\n",
      "|Wednesday|91.57283771155643|\n",
      "|  Tuesday|89.73922367574522|\n",
      "|   Friday|90.44266729389628|\n",
      "| Thursday| 91.4947995556902|\n",
      "|   Monday|89.11023872679046|\n",
      "+---------+-----------------+\n",
      "\n",
      "Average dwell time on the weekend: \n",
      "+--------+------------------+\n",
      "| weekEnd|   avg(dwell-time)|\n",
      "+--------+------------------+\n",
      "|Saturday|116.88253012048193|\n",
      "|  Sunday|106.49005681818181|\n",
      "+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "averageDwellTimePro()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.SparkContext@7136e0e0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/*8. Do the average dwell times computed in #7 indicate any difference in users behavior?*/\n",
    "\n",
    "/*\n",
    "1. Average behaviour each hour does not specify any specific difference in users behaviour\n",
    "\n",
    "2. Average behaviour each day of week specifies that the users' average dwell-time is highest on Sunday and lowest on Tuesdays\n",
    "\n",
    "3. Average behaviour each week-days vs. weekend specifies that the users' average dwell-time is higher on weekend than on weekdays.\n",
    "*/\n",
    "\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
